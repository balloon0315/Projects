{"cells":[{"metadata":{},"cell_type":"markdown","source":"Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\n\nWe may also want to develop some early understanding about the domain of our problem. This is described on the Kaggle competition description page.\n\nOn April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\nAlthough there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class."},{"metadata":{},"cell_type":"markdown","source":"* Question or problem definition.\n* Acquire training and testing data.\n* Wrangle, prepare, cleanse the data.\n* Analyze, identify patterns, and explore the data.\n* Model, predict and solve the problem.\n* Visualize, report, and present the problem solving steps and final solution.\n* Supply or submit the results."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Machine Learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/titanic/test.csv\")\ntrain_df = pd.read_csv(\"../input/titanic/train.csv\")\ncombine = [train_df,test_df]\ntrain_df.info()\nprint('_'*40)\ntest_df.info()\ntrain_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Categorical:** Survived, Sex, and Embarked. Ordinal: Pclass.\n* **Continous:** Age, Fare. Discrete: SibSp, Parch.\n* Seven features are integer or floats. Six in case of test dataset.\n* Five features are strings (object)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check Missing values\nprint('Train columns with null values:\\n', train_df.isnull().sum())\nprint(\"-\"*40)\nprint('Test columns with null values:\\n', test_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Train columns with null values:** Cabin > Age > Embarked \n* **Test columns with null values:** Cabin > Age > Fare\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The distribution of numerical feature values across the samples:**\n* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n* The sample sruvival rate is around 38%.\n* Fares varied significantly with few passengers (<1%) paying as high as $512.\n* Few elderly passengers (<1%) within age range 65-80."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe(include='O')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The distribution of categorical features:**\n* Names are unique across the dataset (count=unique=891)\n* Sex variable as two possible values with 65% male (top=male, freq=577/count=891).\n* Ticket feature has high ratio (22%) of duplicate values (unique=681).\n* Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.(147/204)\n* Embarked takes three possible values. S port used by most passengers (top=S)\n"},{"metadata":{},"cell_type":"markdown","source":"Next, we consider and explore several assumption factors."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most passengers are in 15-35 age range\n* Large number of passengers in age range(15-30) didn't survive.\n* Infants (Age <=4) had high survival rate.\n* Oldest passengers (Age = 80) survived."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_df,\n                height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In all classes, most survived passenegers are female.\n* The survival rate of female is much higher than males'.\n* The survival rate decreased from class 1 to class 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Pclass=3 had most passengers, however most did not survive..\n* Most passengers in Pclass=1 survived."},{"metadata":{},"cell_type":"markdown","source":"# Data Processing and Exploration\n\nmissing data, new features, converting."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute missing data; Drop columns.\nfor dataset in combine:    \n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \ndrop_column = ['PassengerId','Cabin', 'Ticket']\ntrain_df.drop(drop_column, axis=1, inplace = True)\ntest_id=test_df['PassengerId']\ntest_df.drop(drop_column, axis=1, inplace = True)\nprint('Train columns with null values:\\n', train_df.isnull().sum())\nprint(\"-\"*40)\nprint('Test columns with null values:\\n', test_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analyze by pivoting features**\n\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in train_df.columns[1:9]:\n    if train_df[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(train_df[[x,\"Survived\"]].groupby(x, as_index=False).mean().sort_values(by='Survived', ascending=False))\n        print('-'*40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Pclass : ** We observe significant correlation (>0.5) among Pclass=1 and Survived.We decide to include this feature in our model.\n* **Name : ** Name values were mixed texture, we can extract new feature \"Title\" based on this.\n* **Sex : ** Sex=female had very high survival rate at 74%.\n* **SibSp and Parch : ** These features had zero correlation for certain values. We can derive features from these individual features.\n* **Embarked : ** Embarked=C had higher survival rate at 55%.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Name --> Title\n#extract these.count less than 10 with title = \"Rare\"\nfor dataset in combine:  \n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    title_names = (dataset['Title'].value_counts() < 10)\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Rare' if title_names.loc[x] == True else x)\n    dataset.drop(['Name'], axis=1, inplace = True)\n\nprint('Train Count of Titles:\\n',train_df['Title'].value_counts())\nprint('-'*40)\nprint('Test Count of Titles:\\n',test_df['Title'].value_counts())\nprint('-'*40)\nprint('Train title with null values:\\n', train_df[\"Title\"].isnull().sum())\nprint(\"-\"*40)\nprint('Test title with null values:\\n', test_df[\"Title\"].isnull().sum())\nprint(\"-\"*40)\nprint(train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create new feature (Family Size/ IsAlone) combining existing features (SibSp/ Parch) \nfor dataset in combine:\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1   #Discrete variables\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n    drop_column = ['SibSp','Parch','FamilySize']\n    dataset.drop(drop_column, axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Fare and Age bands (reduce the effects of minor observation errors.)\nfor dataset in combine:\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\nprint(train_df[['FareBin', 'Survived']].groupby(['FareBin'], as_index=False).mean().sort_values(by='FareBin', ascending=True))\nprint(\"-\"*40)\nprint(train_df[['AgeBin', 'Survived']].groupby(['AgeBin'], as_index=False).mean().sort_values(by='AgeBin', ascending=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace Fare and Age with ordinals based on these bands.\nfor dataset in combine:    \n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n #--------------------------------------------------------------------------------------   \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n    dataset['Age'] = dataset['Age'].astype(int)\n #--------------------------------------------------------------------------------------      \n    drop_column = ['FareBin','AgeBin']\n    dataset.drop(drop_column, axis=1, inplace = True)\n\ntrain_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert the categorical values (Title/ Sex/ Embarked) to ordinal.\n#That categorical data is defined as variables with a finite set of label values. \n#That most machine learning algorithms require numerical input and output variables. \n#That an integer and one hot encoding is used to convert categorical data to integer data.\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}).astype(int)\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \ntrain_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation\nfor x in train_df.columns[1:8]:\n    if train_df[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(train_df[[x,\"Survived\"]].groupby(x, as_index=False).mean().sort_values(by='Survived', ascending=False))\n        print('-'*40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Positive coefficients increase the log-odds of the response (and thus increase the probability).\n#Negative coefficients decrease the log-odds of the response (and thus decrease the probability).\ncorrelation = train_df.corr()\nplt.figure(figsize=(10,8))\nmask = np.zeros_like(correlation)#https://seaborn.pydata.org/generated/seaborn.heatmap.html\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(correlation,linewidths=.3,annot=True,mask=mask,cmap=\"YlGnBu\",cbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Sex had highest correlation with Survived.\n* Title was second highest positive correlation. and it's related with Sex and Fare.\n* Pclass had negative correlation with survived."},{"metadata":{"trusted":true},"cell_type":"code","source":"#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(14,10))\nlist1=['Pclass', 'Sex', 'Age', 'Fare', 'Embarked','IsAlone'];list2=[0,0,0,1,1,1];list3=[0,1,2,0,1,2]\nfor (x,y,z) in zip(list1,list2,list3): \n    sns.barplot(x = x, y = 'Survived', data=train_df, ax = saxis[y,z])\n    print(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above graph show that in each conditions, which type of passenger had higher survival rate."},{"metadata":{},"cell_type":"markdown","source":"# Model, predict and solve\n\nThe purpose of machine learning is to solve human problems.Machine learning can be categorized as: supervised learning, unsupervised learning, and reinforced learning. \n\n    Supervised learning is where you train the model by presenting it a training dataset that includes the correct answer. \n\n    Unsupervised learning is where you train the model using a training dataset that does not include the correct answer.\n   \n\nWe are doing supervised machine learning, because we are training our algorithm by presenting it with a set of features and their corresponding target.There are many machine learning algorithms, however they can be reduced to four categories: classification, regression, clustering, or dimensionality reduction, depending on your target variable and data modeling goals.\n\nWe want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. So our problem is a classification and regression problem. We can narrow down our choice of models to a few. These include:\n\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forest\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.copy()\nX_train.shape, Y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nlogreg = LogisticRegression()\nY_pred1 = logreg.fit(X_train, Y_train).predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n# KNN\nknn = KNeighborsClassifier(n_neighbors = 4)\nY_pred2 = knn.fit(X_train, Y_train).predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n# Support Vector Machines\nsvm = SVC()\nY_pred3 = svm.fit(X_train, Y_train).predict(X_test)\nacc_svm = round(svm.score(X_train, Y_train) * 100, 2)\n# Naive Bayes classifier\nnb = GaussianNB()\nY_pred4 = nb.fit(X_train, Y_train).predict(X_test)\nacc_nb = round(nb.score(X_train, Y_train) * 100, 2)\n# Decision Tree\ndecision_tree = DecisionTreeClassifier()\nY_pred5 = decision_tree.fit(X_train, Y_train).predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n# Random Forrest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nY_pred6 = random_forest.fit(X_train, Y_train).predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n#--------------------------------------------------------------------------\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression','KNN','Support Vector Machines','Naive Bayes','Decision Tree', 'Random Forest'],\n    'Score': [acc_log, acc_knn, acc_svm,  acc_nb,acc_decision_tree,acc_random_forest]})\nmodels=models.sort_values(by='Score', ascending=False)\nmodels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='Score', y = 'Model', data = models)\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_id,\n        \"Survived\": Y_pred1\n    })\n\nsubmission.to_csv('Submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}